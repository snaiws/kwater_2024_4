{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSe_sd9IS6Vl"
      },
      "source": [
        "# 상수도 관망 이상 감지를 위한 AI 모델 개발\n",
        "- '2024 상수도 관망 이상 감지 AI 경진대회'는 데이터와 AI 기술을 활용하여 상수도 관망의 이상 징후와 누수를 탐지하는 것을 목표로 합니다. <br> 이 대회는 복잡한 상수도 관망 시스템에서 발생하는 다양한 데이터를 효과적으로 분석하고 실시간으로 이상을 감지할 수 있는 AI 알고리즘 개발에 초점을 맞추고 있습니다.\n",
        "- 이 대회의 궁극적 목적은 참가자들의 데이터 기반 이상 감지 역량을 강화하고, AI 기술이 실제 상수도 관리 시스템과 의사결정 과정에 어떻게 기여할 수 있는지 탐구하는 것입니다. <br> 개발된 AI 모델은 상수관망 디지털트윈 및 Water-Net 시스템에 통합되어 더욱 효율적이고 정확한 상수도 관리를 가능하게 할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQk3GTCFS6Vm"
      },
      "source": [
        "## Baseline\n",
        "- 본 베이스라인은 참가자분들께서 가장 기초적인 방법론을 통해 전체 프로세스를 이해하고 경험하실 수 있도록 구성되었습니다. <br> 데이터 로드부터 모델 학습, 추론, 그리고 최종 제출까지 이어지는 end-to-end 파이프라인의 경험을 제공하기 위함입니다.\n",
        "- LSTM과 오토인코더를 활용한 시계열 이상치 탐지 파이프라인을 구현하고 있으며, 시계열 데이터의 특성을 고려해 윈도우 기반으로 학습 데이터를 생성하고,<br> 정상 패턴을 학습하여 이를 기반으로 이상치를 탐지하는 과정을 포함하고 있습니다.\n",
        "- 제공되는 베이스라인 방법론이 본 task를 해결할 수 있는 유일한 방안은 아닙니다! 베이스라인 외에도 다양하고 유의미한 방법론들이 존재할테니,<br> 참가자 여러분들이 자유롭게 탐색하고 발전시켜 나가시기를 바랍니다.\n",
        "- 데이터의 관망 정보를 활용하여 GNN(Graph Neural Networks) 등을 적용한다면, 더 나은 성능을 기대할 수도 있습니다.<br> 💡 여러분의 독창적인 아이디어로 의미 있는 성과를 만들어보세요! 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXgZfWx9S6Vm"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SAcY8IFzS6Vn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import List, Dict, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xku5JgMqS6Vn"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "bff3DT9WTHYc",
        "outputId": "a89326a4-d73e-4c55-ce09-af7616347fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATADIR = \"/content/gdrive/MyDrive/kwater\""
      ],
      "metadata": {
        "id": "A9ZIDCJvTSku"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AQvJaahVS6Vn",
        "outputId": "0a247e8c-6e77-4340-b03b-f7eaf65d8c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/kwater/train/TRAIN_A.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-75debcd94b2e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train/TRAIN_A.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train/TRAIN_B.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/kwater/train/TRAIN_A.csv'"
          ]
        }
      ],
      "source": [
        "df_A = pd.read_csv(os.path.join(DATADIR,\"train/TRAIN_A.csv\"))\n",
        "df_B = pd.read_csv(os.path.join(DATADIR,\"train/TRAIN_B.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rNVPcyES6Vn"
      },
      "source": [
        "# Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmE--dQgS6Vn"
      },
      "outputs": [],
      "source": [
        "\n",
        "config = {\n",
        "    \"WINDOW_GIVEN\"      : 10080,   # 1 week\n",
        "    \"BATCH_SIZE\"        : 64,\n",
        "    \"HIDDEN_DIM_LSTM\"   : 1024,\n",
        "    \"NUM_LAYERS\"        : 1,\n",
        "    \"EPOCHS\"            : 3,\n",
        "    \"LEARNING_RATE\"     : 1e-3,\n",
        "    \"DEVICE\"            : \"cuda\",\n",
        "    \"DROPOUT\"           : 0.2\n",
        "}\n",
        "\n",
        "CFG = SimpleNamespace(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmBSkLmeS6Vn"
      },
      "source": [
        "# Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FR8wzqS6Vn"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, stride: int = 1, inference: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: 입력 데이터프레임\n",
        "            stride: 윈도우 스트라이드\n",
        "            inference: 추론 모드 여부\n",
        "        \"\"\"\n",
        "        self.inference = inference\n",
        "        self.column_names = df.filter(regex='^P\\\\d+$').columns.tolist()\n",
        "        self.file_ids = df['file_id'].values if 'file_id' in df.columns else None\n",
        "\n",
        "        if inference:\n",
        "            self.values = df[self.column_names].values.astype(np.float32)\n",
        "            self._prepare_inference_data()\n",
        "        else:\n",
        "            self._prepare_training_data(df, stride)\n",
        "\n",
        "    def _normalize_columns(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"벡터화된 열 정규화\"\"\"\n",
        "        mins = data.min(axis=0, keepdims=True)\n",
        "        maxs = data.max(axis=0, keepdims=True)\n",
        "\n",
        "        # mins와 maxs가 같으면 전체를 0으로 반환\n",
        "        is_constant = (maxs == mins)\n",
        "        if np.any(is_constant):\n",
        "            normalized_data = np.zeros_like(data)\n",
        "            normalized_data[:, is_constant.squeeze()] = 0\n",
        "            return normalized_data\n",
        "\n",
        "        # 정규화 수행\n",
        "        return (data - mins) / (maxs - mins)\n",
        "\n",
        "    def _prepare_inference_data(self) -> None:\n",
        "        \"\"\"추론 데이터 준비 - 단일 시퀀스\"\"\"\n",
        "        self.normalized_values = self._normalize_columns(self.values)\n",
        "\n",
        "    def _prepare_training_data(self, df: pd.DataFrame, stride: int) -> None:\n",
        "        \"\"\"학습 데이터 준비 - 윈도우 단위\"\"\"\n",
        "        self.values = df[self.column_names].values.astype(np.float32)\n",
        "\n",
        "        # 시작 인덱스 계산 (stride 적용)\n",
        "        potential_starts = np.arange(0, len(df) - CFG.WINDOW_GIVEN, stride)\n",
        "\n",
        "        # 각 윈도우의 마지막 다음 지점(window_size + 1)이 사고가 없는(0) 경우만 필터링\n",
        "        accident_labels = df['anomaly'].values\n",
        "        valid_starts = [\n",
        "            idx for idx in potential_starts\n",
        "            if idx + CFG.WINDOW_GIVEN < len(df) and  # 범위 체크\n",
        "            accident_labels[idx + CFG.WINDOW_GIVEN] == 0  # 윈도우 다음 지점 체크\n",
        "        ]\n",
        "        self.start_idx = np.array(valid_starts)\n",
        "\n",
        "        # 유효한 윈도우들만 추출하여 정규화\n",
        "        windows = np.array([\n",
        "            self.values[i:i + CFG.WINDOW_GIVEN]\n",
        "            for i in self.start_idx\n",
        "        ])\n",
        "\n",
        "        # (윈도우 수, 윈도우 크기, 특성 수)로 한번에 정규화\n",
        "        self.input_data = np.stack([\n",
        "            self._normalize_columns(window) for window in windows\n",
        "        ])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.inference:\n",
        "            return len(self.column_names)\n",
        "        return len(self.start_idx) * len(self.column_names)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Union[str, torch.Tensor]]:\n",
        "        if self.inference:\n",
        "            col_idx = idx\n",
        "            col_name = self.column_names[col_idx]\n",
        "            col_data = self.normalized_values[:, col_idx]\n",
        "            file_id = self.file_ids[idx] if self.file_ids is not None else None\n",
        "            return {\n",
        "                \"column_name\": col_name,\n",
        "                \"input\": torch.from_numpy(col_data).unsqueeze(-1),  # (time_steps, 1)\n",
        "                \"file_id\": file_id\n",
        "            }\n",
        "\n",
        "        window_idx = idx // len(self.column_names)\n",
        "        col_idx = idx % len(self.column_names)\n",
        "\n",
        "        return {\n",
        "            \"column_name\": self.column_names[col_idx],\n",
        "            \"input\": torch.from_numpy(self.input_data[window_idx, :, col_idx]).unsqueeze(-1)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzQX2Y0S6Vo"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_js6GfuxS6Vo"
      },
      "outputs": [],
      "source": [
        "train_dataset_A = TimeSeriesDataset(df_A, stride=60)\n",
        "train_dataset_B = TimeSeriesDataset(df_B, stride=60)\n",
        "train_dataset_A_B = torch.utils.data.ConcatDataset([train_dataset_A, train_dataset_B])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_A_B,\n",
        "                                            batch_size=CFG.BATCH_SIZE,\n",
        "                                            shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUpxRMwPS6Vo"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7uqJJENS6Vo"
      },
      "outputs": [],
      "source": [
        "class LSTM_AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM_AE, self).__init__()\n",
        "\n",
        "        # LSTM feature extractor\n",
        "        self.lstm_feature = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=CFG.HIDDEN_DIM_LSTM,\n",
        "            num_layers=CFG.NUM_LAYERS,\n",
        "            batch_first=True,\n",
        "            dropout=CFG.DROPOUT if CFG.NUM_LAYERS > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Encoder modules\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM, CFG.HIDDEN_DIM_LSTM//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM//4, CFG.HIDDEN_DIM_LSTM//8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Decoder modules\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM//8, CFG.HIDDEN_DIM_LSTM//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM//4, CFG.HIDDEN_DIM_LSTM),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm_feature(x)\n",
        "        last_hidden = hidden[-1]  # (batch, hidden_dim)\n",
        "\n",
        "        # AE\n",
        "        latent_z = self.encoder(last_hidden)\n",
        "        reconstructed_hidden = self.decoder(latent_z)\n",
        "\n",
        "        return last_hidden, reconstructed_hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFxTXJyS6Vo"
      },
      "source": [
        "# Train AE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urxpvcYSS6Vo"
      },
      "outputs": [],
      "source": [
        "def train_AE(model, train_loader, optimizer, criterion, n_epochs, device):\n",
        "    train_losses = []\n",
        "    best_model = {\n",
        "        \"loss\": float('inf'),\n",
        "        \"state\": None,\n",
        "        \"epoch\": 0\n",
        "    }\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as t:\n",
        "            for batch in t:\n",
        "                inputs = batch[\"input\"].to(device)\n",
        "                original_hidden, reconstructed_hidden = model(inputs) # [ Batch_size, HIDDEN_DIM_LSTM ]\n",
        "\n",
        "                loss = criterion(reconstructed_hidden, original_hidden)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                t.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, Average Train Loss: {avg_epoch_loss:.8f}\")\n",
        "\n",
        "        if avg_epoch_loss < best_model[\"loss\"]:\n",
        "            best_model[\"state\"] = model.state_dict()\n",
        "            best_model[\"loss\"] = avg_epoch_loss\n",
        "            best_model[\"epoch\"] = epoch + 1\n",
        "\n",
        "    return train_losses, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWvl6VCaS6Vo"
      },
      "outputs": [],
      "source": [
        "MODEL = LSTM_AE().cuda()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(MODEL.parameters(), lr=CFG.LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt35UdXnS6Vo",
        "outputId": "4eaeb2e3-18b3-4f4b-e6ee-7367bfe94cb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3:   0%|          | 0/314 [00:28<?, ?batch/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 12.31 GiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 13.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_AE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mtrain_AE\u001b[0;34m(model, train_loader, optimizer, criterion, n_epochs, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[1;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m     original_hidden, reconstructed_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [ Batch_size, HIDDEN_DIM_LSTM ]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed_hidden, original_hidden)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mLSTM_AE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (batch, hidden_dim)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# AE\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 13.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "train_losses, best_model = train_AE(MODEL,\n",
        "                                    train_loader=train_loader,\n",
        "                                    optimizer=optimizer,\n",
        "                                    criterion=criterion,\n",
        "                                    n_epochs=CFG.EPOCHS,\n",
        "                                    device=CFG.DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SI-3yDNS6Vp",
        "outputId": "d32e9a8d-d40f-4255-fbde-72cff28a31f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INFER_MODEL = LSTM_AE().cuda()\n",
        "INFER_MODEL.load_state_dict(best_model[\"state\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNKR2ftBS6Vp"
      },
      "source": [
        "# Define threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8dt_d4_S6Vp",
        "outputId": "3262022f-6a80-4455-8eea-068a70b7af75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 314/314 [02:20<00:00,  2.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold calculated and saved: 1.415979906660425e-08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def calculate_and_save_threshold(MODEL, train_loader, percentile=98):\n",
        "    MODEL.eval()\n",
        "    train_errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader):\n",
        "            inputs = batch[\"input\"].to(CFG.DEVICE)\n",
        "            original_hidden, reconstructed_hidden = MODEL(inputs)\n",
        "            mse_errors = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
        "            train_errors.extend(mse_errors)\n",
        "\n",
        "    threshold = np.percentile(train_errors, percentile)\n",
        "\n",
        "    print(f\"Threshold calculated and saved: {threshold}\")\n",
        "    return threshold\n",
        "\n",
        "THRESHOLD = calculate_and_save_threshold(INFER_MODEL, train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWJf8opmS6Vp"
      },
      "source": [
        "# Inference & Detect Anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfIzH-_uS6Vp"
      },
      "outputs": [],
      "source": [
        "def inference_test_files(MODEL, batch, device='cuda'):\n",
        "    MODEL.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = batch[\"input\"].to(device)\n",
        "        original_hidden, reconstructed_hidden = MODEL(inputs)\n",
        "        reconstruction_loss = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
        "    return reconstruction_loss\n",
        "\n",
        "def detect_anomaly(MODEL, test_directory):\n",
        "    test_files = [f for f in os.listdir(test_directory) if f.startswith(\"TEST\") and f.endswith(\".csv\")]\n",
        "    test_datasets = []\n",
        "    all_test_data = []\n",
        "\n",
        "    for filename in tqdm(test_files, desc='Processing test files'):\n",
        "        test_file = os.path.join(test_directory, filename)\n",
        "        df = pd.read_csv(test_file)\n",
        "        df['file_id'] = filename.replace('.csv', '')\n",
        "        individual_df = df[['timestamp', 'file_id'] + df.filter(like='P').columns.tolist()]\n",
        "        individual_dataset = TimeSeriesDataset(individual_df, inference=True)\n",
        "        test_datasets.append(individual_dataset)\n",
        "\n",
        "        all_test_data.append(df)\n",
        "\n",
        "    combined_dataset = torch.utils.data.ConcatDataset(test_datasets)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        combined_dataset,\n",
        "        batch_size=256,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    reconstruction_errors = []\n",
        "    for batch in tqdm(test_loader):\n",
        "        reconstruction_loss = inference_test_files(MODEL, batch, CFG.DEVICE)\n",
        "\n",
        "        for i in range(len(reconstruction_loss)):\n",
        "            reconstruction_errors.append({\n",
        "                \"ID\": batch[\"file_id\"][i],\n",
        "                \"column_name\": batch[\"column_name\"][i],\n",
        "                \"reconstruction_error\": reconstruction_loss[i]\n",
        "            })\n",
        "\n",
        "    errors_df = pd.DataFrame(reconstruction_errors)\n",
        "\n",
        "    flag_columns = []\n",
        "    for column in sorted(errors_df['column_name'].unique()):\n",
        "        flag_column = f'{column}_flag'\n",
        "        errors_df[flag_column] = (errors_df.loc[errors_df['column_name'] == column, 'reconstruction_error'] > THRESHOLD).astype(int)\n",
        "        flag_columns.append(flag_column)\n",
        "\n",
        "    errors_df_pivot = errors_df.pivot_table(index='ID',\n",
        "                                          columns='column_name',\n",
        "                                          values=flag_columns,\n",
        "                                          aggfunc='first')\n",
        "    errors_df_pivot.columns = [f'{col[1]}' for col in errors_df_pivot.columns]\n",
        "    errors_df_flat = errors_df_pivot.reset_index()\n",
        "\n",
        "    errors_df_flat['flag_list'] = errors_df_flat.loc[:, 'P1':'P' + str(len(flag_columns))].apply(lambda x: x.tolist(), axis=1).apply(lambda x: [int(i) for i in x])\n",
        "    return errors_df_flat[[\"ID\", \"flag_list\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQVtrTq-S6Vp",
        "outputId": "d243785b-81ff-4824-82fd-708ffbf945e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing test files: 100%|██████████| 2920/2920 [00:46<00:00, 62.50it/s]\n",
            "100%|██████████| 92/92 [01:10<00:00,  1.30it/s]\n",
            "Processing test files: 100%|██████████| 2738/2738 [00:36<00:00, 74.78it/s]\n",
            "100%|██████████| 65/65 [00:50<00:00,  1.30it/s]\n"
          ]
        }
      ],
      "source": [
        "C_list = detect_anomaly(INFER_MODEL, test_directory=\"/workspace/Storage/kwater_2024_4/Data/raw/test/C\")\n",
        "D_list = detect_anomaly(INFER_MODEL, test_directory=\"/workspace/Storage/kwater_2024_4/Data/raw/test/D\")\n",
        "C_D_list = pd.concat([C_list, D_list])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFzII0MS6Vp"
      },
      "source": [
        "# Prepare Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0oNMoDaS6Vp"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
        "# 매핑된 값으로 업데이트하되, 매핑되지 않은 경우 기존 값 유지\n",
        "flag_mapping = C_D_list.set_index(\"ID\")[\"flag_list\"]\n",
        "sample_submission[\"flag_list\"] = sample_submission[\"ID\"].map(flag_mapping).fillna(sample_submission[\"flag_list\"])\n",
        "\n",
        "sample_submission.to_csv(\"./baseline_submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kwater",
      "language": "python",
      "name": "kwater"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}