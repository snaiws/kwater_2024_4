{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSe_sd9IS6Vl"
      },
      "source": [
        "# ìƒìˆ˜ë„ ê´€ë§ ì´ìƒ ê°ì§€ë¥¼ ìœ„í•œ AI ëª¨ë¸ ê°œë°œ\n",
        "- '2024 ìƒìˆ˜ë„ ê´€ë§ ì´ìƒ ê°ì§€ AI ê²½ì§„ëŒ€íšŒ'ëŠ” ë°ì´í„°ì™€ AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìƒìˆ˜ë„ ê´€ë§ì˜ ì´ìƒ ì§•í›„ì™€ ëˆ„ìˆ˜ë¥¼ íƒì§€í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. <br> ì´ ëŒ€íšŒëŠ” ë³µì¡í•œ ìƒìˆ˜ë„ ê´€ë§ ì‹œìŠ¤í…œì—ì„œ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ìƒì„ ê°ì§€í•  ìˆ˜ ìˆëŠ” AI ì•Œê³ ë¦¬ì¦˜ ê°œë°œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "- ì´ ëŒ€íšŒì˜ ê¶ê·¹ì  ëª©ì ì€ ì°¸ê°€ìë“¤ì˜ ë°ì´í„° ê¸°ë°˜ ì´ìƒ ê°ì§€ ì—­ëŸ‰ì„ ê°•í™”í•˜ê³ , AI ê¸°ìˆ ì´ ì‹¤ì œ ìƒìˆ˜ë„ ê´€ë¦¬ ì‹œìŠ¤í…œê³¼ ì˜ì‚¬ê²°ì • ê³¼ì •ì— ì–´ë–»ê²Œ ê¸°ì—¬í•  ìˆ˜ ìˆëŠ”ì§€ íƒêµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. <br> ê°œë°œëœ AI ëª¨ë¸ì€ ìƒìˆ˜ê´€ë§ ë””ì§€í„¸íŠ¸ìœˆ ë° Water-Net ì‹œìŠ¤í…œì— í†µí•©ë˜ì–´ ë”ìš± íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ìƒìˆ˜ë„ ê´€ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•  ê²ƒì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQk3GTCFS6Vm"
      },
      "source": [
        "## Baseline\n",
        "- ë³¸ ë² ì´ìŠ¤ë¼ì¸ì€ ì°¸ê°€ìë¶„ë“¤ê»˜ì„œ ê°€ì¥ ê¸°ì´ˆì ì¸ ë°©ë²•ë¡ ì„ í†µí•´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ê³  ê²½í—˜í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. <br> ë°ì´í„° ë¡œë“œë¶€í„° ëª¨ë¸ í•™ìŠµ, ì¶”ë¡ , ê·¸ë¦¬ê³  ìµœì¢… ì œì¶œê¹Œì§€ ì´ì–´ì§€ëŠ” end-to-end íŒŒì´í”„ë¼ì¸ì˜ ê²½í—˜ì„ ì œê³µí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
        "- LSTMê³¼ ì˜¤í† ì¸ì½”ë”ë¥¼ í™œìš©í•œ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ê³  ìˆìœ¼ë©°, ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•´ ìœˆë„ìš° ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ ,<br> ì •ìƒ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "- ì œê³µë˜ëŠ” ë² ì´ìŠ¤ë¼ì¸ ë°©ë²•ë¡ ì´ ë³¸ taskë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ë°©ì•ˆì€ ì•„ë‹™ë‹ˆë‹¤! ë² ì´ìŠ¤ë¼ì¸ ì™¸ì—ë„ ë‹¤ì–‘í•˜ê³  ìœ ì˜ë¯¸í•œ ë°©ë²•ë¡ ë“¤ì´ ì¡´ì¬í• í…Œë‹ˆ,<br> ì°¸ê°€ì ì—¬ëŸ¬ë¶„ë“¤ì´ ììœ ë¡­ê²Œ íƒìƒ‰í•˜ê³  ë°œì „ì‹œì¼œ ë‚˜ê°€ì‹œê¸°ë¥¼ ë°”ëë‹ˆë‹¤.\n",
        "- ë°ì´í„°ì˜ ê´€ë§ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ GNN(Graph Neural Networks) ë“±ì„ ì ìš©í•œë‹¤ë©´, ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.<br> ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ ë…ì°½ì ì¸ ì•„ì´ë””ì–´ë¡œ ì˜ë¯¸ ìˆëŠ” ì„±ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”! ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXgZfWx9S6Vm"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SAcY8IFzS6Vn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import List, Dict, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xku5JgMqS6Vn"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "bff3DT9WTHYc",
        "outputId": "a89326a4-d73e-4c55-ce09-af7616347fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATADIR = \"/content/gdrive/MyDrive/kwater\""
      ],
      "metadata": {
        "id": "A9ZIDCJvTSku"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AQvJaahVS6Vn",
        "outputId": "0a247e8c-6e77-4340-b03b-f7eaf65d8c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/kwater/train/TRAIN_A.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-75debcd94b2e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train/TRAIN_A.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train/TRAIN_B.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/kwater/train/TRAIN_A.csv'"
          ]
        }
      ],
      "source": [
        "df_A = pd.read_csv(os.path.join(DATADIR,\"train/TRAIN_A.csv\"))\n",
        "df_B = pd.read_csv(os.path.join(DATADIR,\"train/TRAIN_B.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rNVPcyES6Vn"
      },
      "source": [
        "# Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmE--dQgS6Vn"
      },
      "outputs": [],
      "source": [
        "\n",
        "config = {\n",
        "    \"WINDOW_GIVEN\"      : 10080,   # 1 week\n",
        "    \"BATCH_SIZE\"        : 64,\n",
        "    \"HIDDEN_DIM_LSTM\"   : 1024,\n",
        "    \"NUM_LAYERS\"        : 1,\n",
        "    \"EPOCHS\"            : 3,\n",
        "    \"LEARNING_RATE\"     : 1e-3,\n",
        "    \"DEVICE\"            : \"cuda\",\n",
        "    \"DROPOUT\"           : 0.2\n",
        "}\n",
        "\n",
        "CFG = SimpleNamespace(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmBSkLmeS6Vn"
      },
      "source": [
        "# Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FR8wzqS6Vn"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, stride: int = 1, inference: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
        "            stride: ìœˆë„ìš° ìŠ¤íŠ¸ë¼ì´ë“œ\n",
        "            inference: ì¶”ë¡  ëª¨ë“œ ì—¬ë¶€\n",
        "        \"\"\"\n",
        "        self.inference = inference\n",
        "        self.column_names = df.filter(regex='^P\\\\d+$').columns.tolist()\n",
        "        self.file_ids = df['file_id'].values if 'file_id' in df.columns else None\n",
        "\n",
        "        if inference:\n",
        "            self.values = df[self.column_names].values.astype(np.float32)\n",
        "            self._prepare_inference_data()\n",
        "        else:\n",
        "            self._prepare_training_data(df, stride)\n",
        "\n",
        "    def _normalize_columns(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"ë²¡í„°í™”ëœ ì—´ ì •ê·œí™”\"\"\"\n",
        "        mins = data.min(axis=0, keepdims=True)\n",
        "        maxs = data.max(axis=0, keepdims=True)\n",
        "\n",
        "        # minsì™€ maxsê°€ ê°™ìœ¼ë©´ ì „ì²´ë¥¼ 0ìœ¼ë¡œ ë°˜í™˜\n",
        "        is_constant = (maxs == mins)\n",
        "        if np.any(is_constant):\n",
        "            normalized_data = np.zeros_like(data)\n",
        "            normalized_data[:, is_constant.squeeze()] = 0\n",
        "            return normalized_data\n",
        "\n",
        "        # ì •ê·œí™” ìˆ˜í–‰\n",
        "        return (data - mins) / (maxs - mins)\n",
        "\n",
        "    def _prepare_inference_data(self) -> None:\n",
        "        \"\"\"ì¶”ë¡  ë°ì´í„° ì¤€ë¹„ - ë‹¨ì¼ ì‹œí€€ìŠ¤\"\"\"\n",
        "        self.normalized_values = self._normalize_columns(self.values)\n",
        "\n",
        "    def _prepare_training_data(self, df: pd.DataFrame, stride: int) -> None:\n",
        "        \"\"\"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ - ìœˆë„ìš° ë‹¨ìœ„\"\"\"\n",
        "        self.values = df[self.column_names].values.astype(np.float32)\n",
        "\n",
        "        # ì‹œì‘ ì¸ë±ìŠ¤ ê³„ì‚° (stride ì ìš©)\n",
        "        potential_starts = np.arange(0, len(df) - CFG.WINDOW_GIVEN, stride)\n",
        "\n",
        "        # ê° ìœˆë„ìš°ì˜ ë§ˆì§€ë§‰ ë‹¤ìŒ ì§€ì (window_size + 1)ì´ ì‚¬ê³ ê°€ ì—†ëŠ”(0) ê²½ìš°ë§Œ í•„í„°ë§\n",
        "        accident_labels = df['anomaly'].values\n",
        "        valid_starts = [\n",
        "            idx for idx in potential_starts\n",
        "            if idx + CFG.WINDOW_GIVEN < len(df) and  # ë²”ìœ„ ì²´í¬\n",
        "            accident_labels[idx + CFG.WINDOW_GIVEN] == 0  # ìœˆë„ìš° ë‹¤ìŒ ì§€ì  ì²´í¬\n",
        "        ]\n",
        "        self.start_idx = np.array(valid_starts)\n",
        "\n",
        "        # ìœ íš¨í•œ ìœˆë„ìš°ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ì •ê·œí™”\n",
        "        windows = np.array([\n",
        "            self.values[i:i + CFG.WINDOW_GIVEN]\n",
        "            for i in self.start_idx\n",
        "        ])\n",
        "\n",
        "        # (ìœˆë„ìš° ìˆ˜, ìœˆë„ìš° í¬ê¸°, íŠ¹ì„± ìˆ˜)ë¡œ í•œë²ˆì— ì •ê·œí™”\n",
        "        self.input_data = np.stack([\n",
        "            self._normalize_columns(window) for window in windows\n",
        "        ])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.inference:\n",
        "            return len(self.column_names)\n",
        "        return len(self.start_idx) * len(self.column_names)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Union[str, torch.Tensor]]:\n",
        "        if self.inference:\n",
        "            col_idx = idx\n",
        "            col_name = self.column_names[col_idx]\n",
        "            col_data = self.normalized_values[:, col_idx]\n",
        "            file_id = self.file_ids[idx] if self.file_ids is not None else None\n",
        "            return {\n",
        "                \"column_name\": col_name,\n",
        "                \"input\": torch.from_numpy(col_data).unsqueeze(-1),  # (time_steps, 1)\n",
        "                \"file_id\": file_id\n",
        "            }\n",
        "\n",
        "        window_idx = idx // len(self.column_names)\n",
        "        col_idx = idx % len(self.column_names)\n",
        "\n",
        "        return {\n",
        "            \"column_name\": self.column_names[col_idx],\n",
        "            \"input\": torch.from_numpy(self.input_data[window_idx, :, col_idx]).unsqueeze(-1)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzQX2Y0S6Vo"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_js6GfuxS6Vo"
      },
      "outputs": [],
      "source": [
        "train_dataset_A = TimeSeriesDataset(df_A, stride=60)\n",
        "train_dataset_B = TimeSeriesDataset(df_B, stride=60)\n",
        "train_dataset_A_B = torch.utils.data.ConcatDataset([train_dataset_A, train_dataset_B])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_A_B,\n",
        "                                            batch_size=CFG.BATCH_SIZE,\n",
        "                                            shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUpxRMwPS6Vo"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7uqJJENS6Vo"
      },
      "outputs": [],
      "source": [
        "class LSTM_AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM_AE, self).__init__()\n",
        "\n",
        "        # LSTM feature extractor\n",
        "        self.lstm_feature = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=CFG.HIDDEN_DIM_LSTM,\n",
        "            num_layers=CFG.NUM_LAYERS,\n",
        "            batch_first=True,\n",
        "            dropout=CFG.DROPOUT if CFG.NUM_LAYERS > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Encoder modules\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM, CFG.HIDDEN_DIM_LSTM//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM//4, CFG.HIDDEN_DIM_LSTM//8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Decoder modules\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM//8, CFG.HIDDEN_DIM_LSTM//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(CFG.HIDDEN_DIM_LSTM//4, CFG.HIDDEN_DIM_LSTM),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm_feature(x)\n",
        "        last_hidden = hidden[-1]  # (batch, hidden_dim)\n",
        "\n",
        "        # AE\n",
        "        latent_z = self.encoder(last_hidden)\n",
        "        reconstructed_hidden = self.decoder(latent_z)\n",
        "\n",
        "        return last_hidden, reconstructed_hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFxTXJyS6Vo"
      },
      "source": [
        "# Train AE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urxpvcYSS6Vo"
      },
      "outputs": [],
      "source": [
        "def train_AE(model, train_loader, optimizer, criterion, n_epochs, device):\n",
        "    train_losses = []\n",
        "    best_model = {\n",
        "        \"loss\": float('inf'),\n",
        "        \"state\": None,\n",
        "        \"epoch\": 0\n",
        "    }\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as t:\n",
        "            for batch in t:\n",
        "                inputs = batch[\"input\"].to(device)\n",
        "                original_hidden, reconstructed_hidden = model(inputs) # [ Batch_size, HIDDEN_DIM_LSTM ]\n",
        "\n",
        "                loss = criterion(reconstructed_hidden, original_hidden)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                t.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, Average Train Loss: {avg_epoch_loss:.8f}\")\n",
        "\n",
        "        if avg_epoch_loss < best_model[\"loss\"]:\n",
        "            best_model[\"state\"] = model.state_dict()\n",
        "            best_model[\"loss\"] = avg_epoch_loss\n",
        "            best_model[\"epoch\"] = epoch + 1\n",
        "\n",
        "    return train_losses, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWvl6VCaS6Vo"
      },
      "outputs": [],
      "source": [
        "MODEL = LSTM_AE().cuda()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(MODEL.parameters(), lr=CFG.LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt35UdXnS6Vo",
        "outputId": "4eaeb2e3-18b3-4f4b-e6ee-7367bfe94cb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3:   0%|          | 0/314 [00:28<?, ?batch/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 12.31 GiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 13.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_AE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mtrain_AE\u001b[0;34m(model, train_loader, optimizer, criterion, n_epochs, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[1;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m     original_hidden, reconstructed_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [ Batch_size, HIDDEN_DIM_LSTM ]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed_hidden, original_hidden)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mLSTM_AE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (batch, hidden_dim)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# AE\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kwater-2024-4-F1RZMXvL-py3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 13.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "train_losses, best_model = train_AE(MODEL,\n",
        "                                    train_loader=train_loader,\n",
        "                                    optimizer=optimizer,\n",
        "                                    criterion=criterion,\n",
        "                                    n_epochs=CFG.EPOCHS,\n",
        "                                    device=CFG.DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SI-3yDNS6Vp",
        "outputId": "d32e9a8d-d40f-4255-fbde-72cff28a31f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INFER_MODEL = LSTM_AE().cuda()\n",
        "INFER_MODEL.load_state_dict(best_model[\"state\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNKR2ftBS6Vp"
      },
      "source": [
        "# Define threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8dt_d4_S6Vp",
        "outputId": "3262022f-6a80-4455-8eea-068a70b7af75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [02:20<00:00,  2.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold calculated and saved: 1.415979906660425e-08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def calculate_and_save_threshold(MODEL, train_loader, percentile=98):\n",
        "    MODEL.eval()\n",
        "    train_errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader):\n",
        "            inputs = batch[\"input\"].to(CFG.DEVICE)\n",
        "            original_hidden, reconstructed_hidden = MODEL(inputs)\n",
        "            mse_errors = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
        "            train_errors.extend(mse_errors)\n",
        "\n",
        "    threshold = np.percentile(train_errors, percentile)\n",
        "\n",
        "    print(f\"Threshold calculated and saved: {threshold}\")\n",
        "    return threshold\n",
        "\n",
        "THRESHOLD = calculate_and_save_threshold(INFER_MODEL, train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWJf8opmS6Vp"
      },
      "source": [
        "# Inference & Detect Anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfIzH-_uS6Vp"
      },
      "outputs": [],
      "source": [
        "def inference_test_files(MODEL, batch, device='cuda'):\n",
        "    MODEL.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = batch[\"input\"].to(device)\n",
        "        original_hidden, reconstructed_hidden = MODEL(inputs)\n",
        "        reconstruction_loss = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
        "    return reconstruction_loss\n",
        "\n",
        "def detect_anomaly(MODEL, test_directory):\n",
        "    test_files = [f for f in os.listdir(test_directory) if f.startswith(\"TEST\") and f.endswith(\".csv\")]\n",
        "    test_datasets = []\n",
        "    all_test_data = []\n",
        "\n",
        "    for filename in tqdm(test_files, desc='Processing test files'):\n",
        "        test_file = os.path.join(test_directory, filename)\n",
        "        df = pd.read_csv(test_file)\n",
        "        df['file_id'] = filename.replace('.csv', '')\n",
        "        individual_df = df[['timestamp', 'file_id'] + df.filter(like='P').columns.tolist()]\n",
        "        individual_dataset = TimeSeriesDataset(individual_df, inference=True)\n",
        "        test_datasets.append(individual_dataset)\n",
        "\n",
        "        all_test_data.append(df)\n",
        "\n",
        "    combined_dataset = torch.utils.data.ConcatDataset(test_datasets)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        combined_dataset,\n",
        "        batch_size=256,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    reconstruction_errors = []\n",
        "    for batch in tqdm(test_loader):\n",
        "        reconstruction_loss = inference_test_files(MODEL, batch, CFG.DEVICE)\n",
        "\n",
        "        for i in range(len(reconstruction_loss)):\n",
        "            reconstruction_errors.append({\n",
        "                \"ID\": batch[\"file_id\"][i],\n",
        "                \"column_name\": batch[\"column_name\"][i],\n",
        "                \"reconstruction_error\": reconstruction_loss[i]\n",
        "            })\n",
        "\n",
        "    errors_df = pd.DataFrame(reconstruction_errors)\n",
        "\n",
        "    flag_columns = []\n",
        "    for column in sorted(errors_df['column_name'].unique()):\n",
        "        flag_column = f'{column}_flag'\n",
        "        errors_df[flag_column] = (errors_df.loc[errors_df['column_name'] == column, 'reconstruction_error'] > THRESHOLD).astype(int)\n",
        "        flag_columns.append(flag_column)\n",
        "\n",
        "    errors_df_pivot = errors_df.pivot_table(index='ID',\n",
        "                                          columns='column_name',\n",
        "                                          values=flag_columns,\n",
        "                                          aggfunc='first')\n",
        "    errors_df_pivot.columns = [f'{col[1]}' for col in errors_df_pivot.columns]\n",
        "    errors_df_flat = errors_df_pivot.reset_index()\n",
        "\n",
        "    errors_df_flat['flag_list'] = errors_df_flat.loc[:, 'P1':'P' + str(len(flag_columns))].apply(lambda x: x.tolist(), axis=1).apply(lambda x: [int(i) for i in x])\n",
        "    return errors_df_flat[[\"ID\", \"flag_list\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQVtrTq-S6Vp",
        "outputId": "d243785b-81ff-4824-82fd-708ffbf945e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing test files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2920/2920 [00:46<00:00, 62.50it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [01:10<00:00,  1.30it/s]\n",
            "Processing test files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2738/2738 [00:36<00:00, 74.78it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:50<00:00,  1.30it/s]\n"
          ]
        }
      ],
      "source": [
        "C_list = detect_anomaly(INFER_MODEL, test_directory=\"/workspace/Storage/kwater_2024_4/Data/raw/test/C\")\n",
        "D_list = detect_anomaly(INFER_MODEL, test_directory=\"/workspace/Storage/kwater_2024_4/Data/raw/test/D\")\n",
        "C_D_list = pd.concat([C_list, D_list])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFzII0MS6Vp"
      },
      "source": [
        "# Prepare Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0oNMoDaS6Vp"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
        "# ë§¤í•‘ëœ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë˜, ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ì¡´ ê°’ ìœ ì§€\n",
        "flag_mapping = C_D_list.set_index(\"ID\")[\"flag_list\"]\n",
        "sample_submission[\"flag_list\"] = sample_submission[\"ID\"].map(flag_mapping).fillna(sample_submission[\"flag_list\"])\n",
        "\n",
        "sample_submission.to_csv(\"./baseline_submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kwater",
      "language": "python",
      "name": "kwater"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}